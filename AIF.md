RLAIF已经不是新鲜事了，之前包括Anthropic，谷歌都推出过自己的「AI训AI」的技术


# Meta提出自奖励语言模型

https://mp.weixin.qq.com/s/0Hd3VsmPVMXWATytZQuOiQ

Self-Rewarding Language Models : https://arxiv.org/pdf/2401.10020.pdf

然后研究人员建立一个模型，让它同时拥有两种能力：

指令遵循：给出描述用户请求的提示，能够生成高质量、有帮助（且无害）的响应。

自指令创建：能够按照示例生成和评估新指令，再添加到自己的训练集中。

这两个能力可以为了使模型能够执行自我对齐，即它们是用于使用人工智能反馈（AIF）迭代训练自身的组件。

研究人员使用同一模型的LLM-as-a-Judge能力来评估其自己的候选响应，得分为 r∈ [0, 5]

迭代训练

研究人员的整个过程训练一系列模型。其中每个连续模型t使用由t − 1模型创建的增强训练数据。
因此，研究人员将AIFT(M)定义为使用模型M创建的AI反馈训练数据。
M：基础预训练LLM，没有微调。
M1：用M初始化，然后使用SFT对IFT+EFT种子数据进行微调。
M2：用M1初始化，然后使用DPO用AIFT(M1)数据进行训练。
M3：用M2初始化，然后使用DPO用AIFT(M2)数据进行训练。

https://github.com/lucidrains/self-rewarding-lm-pytorch/tree/main

# 基于“梯度下降”和Beam Search的自动提示优化
https://aclanthology.org/2023.emnlp-main.494.pdf
提出了一种自动优化LLM提示方法——Automatic Prompt Optimization(APO)，。通过自动或半自动程序来帮助人类编写最佳提示。这有助于减少人工努力、提高任务性能，并产生可解释的认知决策过程描述。
- 利用LLM本身去寻找prompt的瑕疵。将语言模型的输出y ^ \hat{y} y^与正确答案（label）y yy还有prompt p pp 一起送入LLM，并通过类似“What is wrong with p pp? ”这样的问题让LLM自己找出prompt的问题所在，并将这次输出当作prompt在“自然语言域”的“梯度”g 。
- 根据梯度g让LLM自己对prompt做调整。把p ,g一起输入给LLM，并通过类似 “Use g to fix p ”的指令让LLM生成新的prompt。原文在此基础之上，还使用LLM多次paraphrase新的prompt从而“拓宽蒙特卡洛搜索空间”。
- 结合Beam search和Bandit selection在生成的新prompt里寻找最优解。每次做完前面所述的1.2.两步以后，就会产生许多候选prompt，文章使用beam search的方式，每次用bandit selection找到比较好的几个prompt，在此基础之上继续迭代，若干次后挑选其中最好的prompt。

在4个数据集上做了实验，都是偏向网络安全类的，而且都是分类问题。看上去是吊打Monte-Carlo (MC)，Reinforcement Learning (RL)和AutoGPT。
- Jailbreak：用户尝试绕过LLM的一些安全限制
- Ethos：辨别英语仇恨言论
- Liar：辨别英语fake news
- Sarcasm：辨别阿拉伯语讽刺言论

我们提出了一种通用且非参数化的自动提示优化算法，即基于文本梯度的提示优化（Prompt Optimization with Textual Gradients, ProTeGi），它通过以定向方式对提示进行离散改进来连接这两类研究。
基于文本的test-based Socratic 梯度下降步骤来克服离散优化障碍（zeng 2022），用语言模型反馈替代微分，用语言模型编辑替代反向传播。具体来说，我们使用训练数据的小批量生成自然语言中的“梯度”，即小批量相对于当前提示的缺陷描述，然后根据梯度的相反语义方向编辑当前提示。这些步骤成为对提示空间进行更广泛的束搜索的一部分，通过将束候选选择问题视为最佳臂识别问题的一个实例来提高算法效率（Audibert等人，2010）。


# 用MT-Bench和Chatbot Arena评测LLM as a Judge
https://arxiv.org/pdf/2306.05685.pdf
https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge

由于其广泛的能力和现有基准无法衡量人类偏好的不足，评估大型语言模型 (LLM) 基于聊天助手变得具有挑战性。为此，我们探索使用强大的 LLM 作为裁判来对这些模型进行更开放的问题进行评估。我们研究了 LLM 作为裁判的用法和局限性，包括位置、冗长和自我强化偏差以及有限的推理能力，并提出了缓解其中一些问题的方法。然后，通过引入两个基准测试（多轮问题集 MT-bench 和众包战斗平台 Chatbot Arena）验证了 LLM 裁判与人类偏好之间的共识。我们的结果表明，像 GPT-4 这样的强大 LLM 裁判可以很好地匹配受控和众包的人类偏好，达到超过 80% 的一致性，与人类之间的一致性相同。因此，LLM 作为裁判是一种可扩展且可解释的方式来近似人类偏好，否则获取人类偏好是非常昂贵的。此外，我们通过评估 LLaMA 和 Vicuna 的几个变体来展示我们的基准测试和传统基准测试互相补充。MT-bench 问题、3k 专家投票和 30k 次对话中的人类偏好在 https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge 上公开可用。

在这篇论文中，我们通过与人类评价的黄金标准进行比较来研究 LLM-as-a-judge 方法。我们检查了 LLM-as-a-judge 方法的一些潜在局限性，包括位置偏见、冗长偏见、自我强化偏见以及有限的推理能力。我们证明了一些偏差是可以忽略不计的或可以缓解的。一旦解决，我们的结果证实了来自 3k 个受控专家投票和 3k 个众包的人类投票的结果，即 GPT-4 法官匹配

随着 LLM 的最新进展，基于 LLM 的助手开始在各种任务中展示人工智能的通用性，从写作、聊天到编码。[5, 30, 1, 37]然而，评估它们的广泛能力也变得更具挑战性。尽管有大量针对语言模型的标准数据集，但这些标准数据集主要关注的是对封闭式问题进行短回答的模型评估。由于这些聊天机器人现在可以在多轮对话中精确地遵循用户的指示，并以零样本的方式回答开放式问题，因此当前的标准数据集不足以评估此类功能。现有的基准测试大多属于以下三类。
• 核心知识基准，包括 MMLU [19]、HellaSwag [50]、ARC [9]、Wino-Grande [36]、HumanEval [6]、GSM-8K [10] 和 AGIEval [51]，使用零样本和少量样本基准集评估预训练语言模型的核心能力。它们通常要求语言模型为可自动验证的基准问题生成简短而具体的答案。
• 指令遵循基准，如 Flan [27, 46]、Self-instruct [44]、NaturalInstructions [28]、Super-NaturalInstructions [45]，扩展到略微更开放的问题和更多样化的任务，并用于在指令微调后评估 LLM。
• 会话基准，如 CoQA [35]、MMDialog [15] 和 OpenAssistant [23]，最接近我们的预期用例。然而，它们的问题往往缺乏多样性且复杂性不足，无法挑战最新聊天机器人的能力。

我们提出了三种“ LLM as a judge” 的变体。它们可以独立或结合使用：
• 对比评估。 一个 LLM 判定器被呈现一个问题和两个答案，然后被要求判断哪个更好或者宣布平局。 使用的提示如图 5 (附录) 所示。
• 单一答案评分。 或者，要求 LLM 判定器直接为单个答案分配分数。 此场景中使用的提示在附录 6 中。
• 参考引导评分。在某些情况下，如果适用的话，提供参考解决方案可能是有益的。我们在图8（附录）中使用的一个数学问题示例提示如下：

作为法官的 LLM 的局限性
* 位置偏见是指，LLM倾向于在某些位置上表现得比其他位置更好。这种偏见并不局限于我们的上下文，在人类决策中[3、34]和其他机器学习领域[22、41]中也有观察到。为了分析位置偏见，我们在 MT-bench 中针对每个第一轮问题构造了两个类似的答案，通过两次调用温度为 0.7 的 GPT-3.5 来实现。然后我们尝试了三个 LLM，使用两种不同的提示：“默认”是我们附录中图 5 所示的默认提示。“重命名”会将我们默认提示中的助手名称更改为查看该偏见是在位置还是名称上。如表 2 所示，我们发现所有这些都表现出强烈的定位偏见。大多数 LLM 判定者赞成第一个位置。Claude-v1 还显示了一个名字偏见，使其偏向“助手A”，如下文所示“重命名”提示。位置偏见可能非常显著。只有 GPT-4 在超过 60% 的情况下输出一致的结果。
* 冗长偏见是指 LLM 更喜欢更长、更冗长的回答，即使它们不如较短的答案清晰、高质量或准确。为了研究这种偏见，我们设计了一个“重复列表”攻击，使用来自MT-bench的模型答案。首先，我们在MT-bench中选择包含编号列表的23个模型答案。然后，通过要求GPT-4对列表进行重新措辞（不添加任何新信息），并将重新措辞后的新列表插入到原始列表的开头来使其变得冗长。例如，如果原始响应包含5个项目，则新响应将包含10个项目，但前5个项目是从原始5个项目重新措辞而来的。示例请参见图12 (附录)。如果LLM判断认为新的响应比旧的更好，则定义攻击成功。表3显示了在这种攻击下LLM裁判员的失败率，表明所有LLM都可能受到冗长偏见的影响，尽管GPT-4的防御能力显著优于其他模型。作为校准，我们发现LLM裁判能够正确地判断相同的答案（即对于两个相同答案，他们总是打平）但他们无法通过更高级的“重复列表”攻击。
* 自我增强偏见。我们从社会认知文献中借用“自我增强偏见”一词 [4] 来描述语言模型判断者可能更喜欢他们自己生成的答案的效果。我们从统计角度来检验这种影响。图 3 (b) 显示了在不同的 LLM 判定者和人类下，六个模型的获胜率（无平局）。与人类相比，我们确实观察到一些裁判偏袒某些模型。例如，GPT-4 偏爱自己，其获胜率高出了 10%; 而Claude-v1 偏爱自己，其获胜率高出了 25%。然而，它们也偏爱其他模型，而 GPT-3.5 不会偏爱自己。由于数据有限且差异较小，我们的研究无法确定这些模型是否表现出自我增强偏差。进行控制研究具有挑战性，因为我们不能轻易地改写响应以适应另一个模型的风格而不改变质量。
* 我们从统计角度来检验这种影响。图 3 (b) 显示了在不同的 LLM 判定者和人类下，六个模型的获胜率（无平局）。与人类相比，我们确实观察到一些裁判偏袒某些模型。例如，GPT-4 偏爱自己，其获胜率高出了 10%; 而Claude-v1 偏爱自己，其获胜率高出了 25%。然而，它们也偏爱其他模型，而 GPT-3.5 不会偏爱自己。由于数据有限且差异较小，我们的研究无法确定这些模型是否表现出自我增强偏差。进行控制研究具有挑战性，因为我们不能轻易地改写响应以适应另一个模型的风格而不改变质量。
  


我们提出了一些方法来解决位置偏见和数学问题评分能力有限的问题。
* 交换位置。可以使用简单的解决方案来解决位置偏差问题。保守的做法是在两次回答中交换两个答案的顺序，只有当一个答案在两种情况下都被偏爱时才宣布获胜。如果交换后结果不一致，则可以宣布平局。另一种更激进的方法是随机分配位置，只要期望正确，在大规模比赛中这种方法非常有效。在下面的实验中，我们使用了保守的方法。
* 少样本裁判。我们评估了少样本示例是否可以提高位置偏置基准的一致性。我们使用类似MT-bench的问题选择三个好的判断示例，GPT-3.5和Vicuna用于生成答案，GPT-4用于生成判断。这些例子涵盖了三种情况：A更好、B更好和平局。如表12（附录）所示，少样本裁判可以显著地将GPT-4的一致性从65.0%提高到77.5%。然而，高一致性并不一定意味着高准确性，而且我们不确定少样本示例是否会引入新的偏差。此外，更长的提示会使API调用昂贵4倍。我们在接下来的实验中默认使用零样提示，但在附录D.2中进行额外研究。
* 链式思维和参考引导型裁判。在第 3.3 节中，我们展示了 LLM 在评分数学和推理问题上的有限能力。为此，我们提出了两种简单的方法来缓解这个问题：链式思维型裁判和参考引导型裁判。链式思维是一种广泛用于提高 LLM 推理能力的技术[47]。我们提出了一种类似的技术，提示 LLM 判分器首先独立回答问题，然后开始打分。具体提示见图 7 (附录)。然而，即使使用了 CoT 提示，我们也发现，在许多情况下，LLM 在解决问题的过程中犯下了与给定答案完全相同的错误（参见图 15 (附录)，这表明 LLM 判分器可能仍然受到上下文的影响。因此，我们提出了一个参考引导方法，在此方法中，我们首先独立生成 LLM 判分器的答案，然后将其显示为判分提示中的参考答案。如表 4 所示，相对于默认提示，该方法显著降低了失败率（从 70% 降低到 15%）。

  我们在 MMLU [19]、Truthful QA[26](MC1) 和 MT-bench (GPT-4 判定者) 上评估了从 LLaMA 派生出的几个模型变体。训练细节请参见附录 E。由于在第 4.2 节中我们已经证明，GPT-4 单一答案评分方法在性能上也很好，因此出于可扩展性和简单性的考虑，我们选择使用 GPT-4 单一答案评分法对 MT-bench 进行评分。我们要求 GPT-4 使用我们的提示模板（图 6 和图 10）为每个回合打分，并报告平均得分为 160 = 80 × 2 个回合。表 8 显示了结果。我们发现，在高质量对话数据集（例如 ShareGPT）上进行微调可以持续提高模型在 MMLU 上的性能，且改进程度与微调的数据量成正比。另一方面，一个小型高质量对话数据集可以快速教会模型一种由 GPT-4（或近似人类）偏好的风格，但不能显著提高 MMLU，如使用仅 4.8M 令牌或 3K 对话进行训练的 Vicuna-7B（选定）。表 8 中没有单个基准可以确定模型质量，这意味着需要进行全面评估。我们的结果表明，使用 LLM 作为裁判来近似人类偏好是非常可行的，并且可能成为未来基准测试的新标准。此外，我们正在定期更新包含更多模型的排行榜。值得注意的是，DynaBench[21] 是一个专门用于动态数据收集和基准测试的研究平台，它与我们的精神是一致的。DynaBench 强调了具有人为干预的人工智能动态数据，以解决静态标准化基准带来的挑战，如饱和度和过拟合。我们的 LLM 作为裁判的方法可以自动化并扩大此类平台的规模。


