# Review
自我指导 (Self-Instruct)：
- 任务多样性：自我指导方法通常使用一组初始提示（prompts）来激发模型生成新的指令。这些提示可能相对简单，涉及基本问题，如 "三原色是什么？" 或 "法国的首都是什么？"，因此任务多样性有限。
- 数据规模：自我指导方法使用初始提示生成新指令，然后删除低质量或过于相似的响应，将剩余的指令重新集成到任务池中，进行进一步迭代。然而，这种方法通常生成数量有限的指令，数据规模可能相对较小。
Evolve-Instruct：
- 任务多样性：Evolve-Instruct 是对自我指导方法的一种改进，它尝试逐渐将初始指令重写成更复杂的版本，以克服自我指导方法中存在的一些任务多样性限制。因此，它可能生成更复杂和多样性的指令。
- 数据规模：尽管Evolve-Instruct改进了任务多样性，但与自我指导方法一样，生成的指令数量仍然受限，数据规模可能有限。
Vicuna 和 Koala：
- 任务多样性：Vicuna 和 Koala 使用用户提供的自然对话作为数据来源，这些对话通常包含更丰富的任务多样性和更自然的指令。因此，它们可能能够捕捉到更广泛的任务类型和复杂性。

  
# Distilling System 2 into System 1
https://arxiv.org/pdf/2407.06023
最近，人们提出了许多方法来使用 LLM 在内部循环中执行更深入的推理，其中它生成中间输出，有时称为 LLM 程序（Schlag等人，2023）。这些包括子问题分解（Perez等人，2020）、自我改进（Madaan等人，2024；Weston和Sukhbaatar，2023；Deng等人，2023a）、自我验证和询问（Press等人，2022；Weng等人，2022；Dhuliu-wala等人，2023）以及诸如树思维的各种搜索技术等（Yao等人，2024；Besta等人，2024）。 
生成中间深度推理的方法：
1. 链式思考（CoT）是一种由GPT-3提出的系统2（System 2）推理技术，其目的是让模型能够执行更复杂的推理任务。CoT的核心思想是让模型通过一系列步骤来解决问题，每个步骤都会产生一些中间结果，这些结果会被传递给下一个步骤，直到得出最终的答案。CoT的优点是可以解决许多不同的问题类型，但它需要更多的计算资源和时间来完成。
2. System 2注意力是另一种系统2（System 2）推理技术，它可以用来控制模型的注意力，以便更好地理解和处理输入。具体来说，System 2注意力可以通过调整模型的注意力权重来过滤掉不必要的信息，从而提高模型的性能。System 2注意力的优点是可以应用于各种不同类型的任务，但它也需要更多的计算资源和时间来完成。
3. 重述和响应（RaR）是一种系统2（System 2）推理技术，它可以帮助模型更好地理解和回答问题。具体来说，RaR可以让模型首先生成一个简短的回答，然后再将其扩展成更详细的回答。这种方法可以使模型更加准确地回答问题，但它也需要更多的计算资源和时间来完成。
4. 分支解决合并（BSM）是一种系统2（System 2）推理技术，它可以帮助模型更好地处理多个任务。具体来说，BSM可以将一个大的任务分解成多个小的任务，并分别处理每个任务。最后，BSM会将所有的结果合并起来，得到最终的结果。这种方法可以使模型更加高效地处理多个任务，但它也需要更多的计算资源和时间来完成。

这四种方法的共同点是它们都是系统2（System 2）推理技术，都需要更多的计算资源和时间来完成。不同之处在于它们的应用场景和实现方式。CoT适用于各种类型的推理任务，但需要更多的计算资源和时间；System 2注意力可以应用于各种不同类型的任务，但同样需要更多的计算资源和时间；RaR可以帮助模型更好地理解和回答问题，但同样需要更多的计算资源和时间；BSM可以帮助模型更好地处理多个任务，但同样需要更多的计算资源和时间。


# Orca LLM
https://www.microsoft.com/en-us/research/project/orca/
Orca LLM：专注于通过对基于示例的推理任务的新数据集进行训练来提高推理能力。它弥补了通用 LLM 和专用推理引擎之间的差距，增强了其解决复杂逻辑问题的能力。算法将复杂的定量和推理问题分解为分步解决方案。
它利用GPT-4的解释追踪、思考过程和复杂指令进行渐进学习，取得了显著的zero-shot推理成绩。
模型通过其庞大的规模和渐进学习方法，尤其是从复杂的解释和指导中学习，以及对大规模多样化的模仿数据的利用，取得了与Vicuna-13B等模型相比的显著优势，特别是在复杂的zero-shot推理基准测试中。这使得Orca成为处理各种任务和挑战的有力工具。
学习从GPT-4获取信号： Orca从GPT-4获取多种信号，其中包括解释追踪、逐步思考过程和其他复杂指令。这些信号有助于Orca理解GPT-4的推理方式和思维过程。具体来说，Orca使用以下方式从GPT-4获取信号：
- Explanation Traces（解释追踪）： GPT-4生成了详细的解释，阐明了它生成回应的推理和思考过程。Orca学习了如何使用这些解释追踪来更好地理解问题和生成回应。
- Step-by-Step Thought Processes（逐步思考过程）： GPT-4提供了逐步的思考过程，从而揭示了问题的逻辑解决方案。Orca学会了模仿这种逐步思考的方式。
- Other Complex Instructions（其他复杂指令）： GPT-4提供了其他复杂指令，这些指令可以引导Orca生成高质量的回应，而不仅仅是简单地模仿GPT-4的风格。
ChatGPT的教师协助： 在Orca的训练过程中，ChatGPT充当了教师模型，提供了指导和帮助。这种教师协助可以帮助Orca更好地理解问题和生成回应。ChatGPT通过以下方式协助Orca：
- 提供指导性对话： ChatGPT参与对话，引导Orca生成回应。这种对话中包含了教育性的要求，例如“像我解释得简单点”或“逐步思考并解释你的回应”。这有助于Orca学会模仿GPT-4的推理方式。
- 提供教育性的提示： ChatGPT还提供了有关如何解决问题、思考和回应的提示和建议，以帮助Orca提高生成的质量和推理能力。
渐进学习方法： Orca采用了渐进学习方法，这意味着它逐渐提高了性能，而不是一次性地训练成熟。这种方法的关键是从大规模和多样化的模仿数据中进行精选和采样。Orca首先学习基本的技能，然后逐渐引入更复杂的指导和信号，如GPT-4的解释追踪和复杂指令。这种渐进学习使Orca能够不断提高其推理和生成能力，最终达到了与GPT-4相媲美的性能水平。
